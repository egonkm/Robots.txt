# -*- coding: utf-8 -*-
"""Robots.ipynb

Automatically generated by Colab.
"""



import requests
from bs4 import BeautifulSoup

def get_links(url):
    """
    Fetches a webpage and extracts all hyperlinks.

    :param url: URL of the webpage to fetch.
    :return: A list of hyperlinks found in the page.
    """
    try:
        # Send a HTTP request to the URL
        response = requests.get(url, timeout=5)
        # Raise an exception if the request was unsuccessful
        response.raise_for_status()

        # Parse the HTML content
        soup = BeautifulSoup(response.text, 'html.parser')

        # Find all anchor tags and extract href values
        links = [a.get('href') for a in soup.find_all('a') if a.get('href')]
        return links
    except requests.RequestException as e:
        print("Error: ", str(e))
        return []
    except Exception as e:
        print(f"error occurred: {e}")
        return []

def new_links(l_links, url):

    s = set()
    for link in l_links:
        if link.startswith(url): continue
        if not link.startswith("http"): continue

        if "://" in link:
            pre = link[0:link.index("//")+2]
            link = link.split("//")[1]
        else:
            pre = ""

        link = pre + link.split("/")[0].split("?")[0].split("#")[0]
        s.add(link)

    return list(s)

import os
from time import sleep

FOLDER = "robots"
SEARCH = "search.txt"

def search():
        
    if not os.path.exists(SEARCH): return

    with open(SEARCH, "r") as f:
        to_search = [line.strip() for line in f]

    while to_search:

        url = to_search.pop()

        # get robots.txt file
        robot_url = url + "/robots.txt"
        robot_file = os.path.join(FOLDER, url.split("//")[1].split("/")[0])

        if os.path.exists(robot_file):
            print("skip:", url)
            continue

        print(len(to_search), url)

        try:
            robot = requests.get(robot_url, timeout=5).text
        except Exception as e:
            print(str(e))
            continue

        if os.path.exists(robot_file): 
            sleep(1)
            continue

        if "<!DOCTYPE" in robot[0:10].upper(): continue

        if "<html>" in robot[0:10].lower(): continue


        with open(robot_file, "w") as f:
            f.write(robot)

        # get links
        links = get_links(url)
        with open(SEARCH, "a") as f:
            for link in new_links(links, url):
                to_search.append(link)
                f.write(link+"\n")

def search_process():

    try:
        search()
    except Exception as e:
        print(str(e))

    print("Done")

import multiprocessing

if __name__ == "__main__":

    processes = []

    for i in range(1):
        
        process = multiprocessing.Process(target=search_process)
        processes.append(process)
        process.start()

    for process in processes: process.join()
    